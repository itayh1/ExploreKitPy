

import os.path
import FileUtils

from ArffSaver import ArffSaver
from Dataset import Dataset
from FileUtils import listFilesInDir
from Loader import Loader
from Logger import Logger
from Properties import Properties
from Serializer import Serializer

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

class MLAttributeManager:

    DATASET_BASED="DatasetBased"
    OA_BASED="OperatorAssignmentBased"
    VALUES_BASED="ValuesBased"
    MERGED_ALL="merged_candidateAttributesData"
    MERGED_NO_VALUE="merged_candidateAttributes_NoValueBased"
    ITERATION = 50000

    '''
    Generates the "background" model that will be used to classify the candidate attributes of the provided dataset.
    The classification model will be generated by combining the information gathered FROM ALL OTHER DATASETS.
    '''
    def getBackgroundClassificationModel(self, dataset: Dataset, includeValueBased: bool):
        backgroundFilePath = self.getBackgroundFilePath(dataset, includeValueBased)
        path = backgroundFilePath


        # If the classification model already exists, load and return it
        if os.path.isfile (path):
            Logger.Info("Background model already exists. Extracting from " + path)
            return self.getClassificationModel(dataset, backgroundFilePath)

        #Otherwise, generate, save and return it (WARNING - takes time)
        else:
            Logger.Info("Background model doesn't exist for dataset " + dataset.name + ". Creating it...")

            # We begin by getting a list of all the datasets that need to participate in the creation of the background model
            self.generateMetaFeaturesInstances(includeValueBased)

            candidateAtrrDirectories = self.getDirectoriesInFolder(Properties.DatasetInstancesFilesLocation)
            self.generateBackgroundARFFFileForDataset(dataset, backgroundFilePath, candidateAtrrDirectories, includeValueBased)

            # now we load the contents of the ARFF file into an Instances object and train the classifier
            data = self.getInstancesFromARFF(backgroundFilePath)
            return self.buildClassifierModel(backgroundFilePath, data)

    def getInstancesFromARFF(self, backgroundFilePath: str):
        # BufferedReader reader = new BufferedReader(new FileReader(backgroundFilePath + ".arff"));
        data = Loader().readArffAsDataframe(backgroundFilePath + '.arff')
        Logger.Info('reading from file ' + backgroundFilePath + '.arff')
        # ArffLoader.ArffReader arffReader = new ArffLoader.ArffReader(reader);
        # Instances data = arffReader.getData();
        # data.setClassIndex(data.numAttributes() - 1);
        return data

    def buildClassifierModel(self, backgroundFilePath: str, data):
        # the chosen classifier
        classifier = RandomForestClassifier()
        # classifier.setNumExecutionSlots(Integer.parseInt(properties.getProperty("numOfThreads")));

        # classifier.buildClassifier(data);
        classifier.fit(data.drop(['class']), data['class'])
        file = backgroundFilePath + '.arff'
        FileUtils.deleteFile(file)

        Logger.Info('Saving classifier model ' + backgroundFilePath)
        self.writeClassifierTobackgroundFile(backgroundFilePath, classifier)
        return classifier

    def writeClassifierTobackgroundFile(backgroundFilePath: str, classifier):
        # now we write the classifier to file prior to returning the object
        Serializer.Serialize(backgroundFilePath, classifier)

    def generateBackgroundARFFFileForDataset(self, dataset:Dataset, backgroundFilePath: str, candidateAttrDirectories: list, includeValueBased: bool):
        addHeader = True
        for candidateAttrDirectory in candidateAttrDirectories:

            if (not candidateAttrDirectory.__contains__(dataset.name)) and FileUtils.listFilesInDir(candidateAttrDirectory)!=None: #none means dir exist

                merged = self.getMergedFile(candidateAttrDirectory,includeValueBased)
                if merged is not None:
                    self.addArffFileContentToTargetFile(backgroundFilePath, merged[0].getAbsolutePath(),addHeader);
                    addHeader = False

                else:
                    instances = [] #List<Instances> instances = new ArrayList<>();
                    for file in listFilesInDir(candidateAttrDirectory):
                        if (file.contains('.arff') and not(not includeValueBased and file.contains(self.VALUES_BASED)) and not(file.contains('merged'))):
                            absFilePath = os.path.abspath(file)
                            instance = Loader().readArffAsDataframe(absFilePath)
                            instances.append(instance)

                        else:
                            Logger.Info(f'Skipping file: {file}')

                    mergedFile = self.mergeInstancesToFile(includeValueBased, candidateAttrDirectory, instances)
                    if mergedFile is None:
                        continue
                    self.addArffFileContentToTargetFile(backgroundFilePath, mergedFile.getAbsolutePath(),addHeader)
                    addHeader = False

    def mergeInstancesToFile(self, includeValueBased: bool, directory: str, instances: list):
        if len(instances) == 0:
            return None

        toMerge = instances[0]
        for i in range(1, len(instances)):
            toMerge = self.mergeDataframes(toMerge, instances[i]) #Instances.mergeInstances(toMerge, instances.get(i));

        saver = ArffSaver()
        saver.setInstances(toMerge)

        if includeValueBased:
            mergedFile = os.path.join(directory, self.MERGED_ALL + '.arff')
        else:
            mergedFile = os.path.join(directory, self.MERGED_NO_VALUE + '.arff')

        saver.setFile(mergedFile)
        saver.writeBatch()
        return mergedFile

    # TODO: replace 'mergeInstances' built-in function
    def mergeDataframes(self, first, second):
        return pd.concat([first, second], axis=1)

    def getMergedFile(self, directory: str, includeValueBased:bool):
        if includeValueBased:
            merged = filter(lambda name: name.contains(self.MERGED_ALL), listFilesInDir(directory))
            if len(merged) == 1:
                return merged

        if not includeValueBased:
            filter(lambda name: name.contains(self.MERGED_NO_VALUE), listFilesInDir(directory))
            if len(merged) == 1:
                return merged

        return None

    def generateMetaFeaturesInstances(self, includeValueBased: bool):
        datasetFilesForBackgroundArray = self.getOriginalBackgroundDatasets()
        for datasetForBackgroundModel in datasetFilesForBackgroundArray:
            possibleFolderName = Properties.DatasetInstancesFilesLocation + datasetForBackgroundModel + '_' + str(Properties.randomSeed)

            if not os.path.isdir(possibleFolderName):
                loader = Loader()
                Logger.Info("Getting candidate attributes for " + datasetForBackgroundModel)
                backgroundDataset = loader.readArff(datasetForBackgroundModel, int(Properties.randomSeed), None, None, 0.66)
                self.createDatasetMetaFeaturesInstances(backgroundDataset, includeValueBased)

    def getDirectoriesInFolder(self, folder:str):
        for (_, directories, _) in os.walk(folder):
            break
        if (directories is None) or (len(directories) == 0):
            raise Exception('getBackgroundClassificationModel -> no directories for meta feature instances')
        return directories

    # Receives a dataset object and generates an Instance object that contains a set of attributes for EACH candidate attribute
    def createDatasetMetaFeaturesInstances(self, dataset: Dataset, includeValueBased: bool):
        directoryForDataset = Properties.DatasetInstancesFilesLocation + dataset.name
        # File[] files;

        if os.path.isdir(directoryForDataset):
            _, _, filenames = next(os.walk(directoryForDataset))
            if (filenames is not None) and (len(filenames)!=0):
                Logger.Info('Candidate attributes for ' + dataset.name + ' were already calculated')
                return

        try:
            os.mkdir(directoryForDataset)
        except Exception as ex:
            Logger.Warn(f'getDatasetMetaFeaturesInstances -> Error creating directory {directoryForDataset}\nError: {ex}')
            return

        # List<String> metadataTypes;
        if includeValueBased:
            # This is the line that activates the (time consuming) background datasets feature generation process
            self.generateTrainingSetDatasetAttributes(dataset)
            metadataTypes = [self.DATASET_BASED, self.OA_BASED, self.VALUES_BASED]
        else:
            # for pre-ranker model
            self.generateTrainingSetDatasetAttributesWithoutValues(dataset)
            metadataTypes = [self.ATASET_BASED, self.OA_BASED]

        self.appendARFFFilesPerMetadataTypeForDataset(directoryForDataset, metadataTypes);

    def getBackgroundFilePath(self, dataset: Dataset, includeValueBased: bool):
        backgroundFilePath = ''
        if includeValueBased:
             backgroundFilePath = Properties.backgroundClassifierLocation + 'background_' + dataset.name + '_' + 'DatasetBased_OperatorAssignmentBased_ValuesBased' + '_classifier_obj'
        else:
             backgroundFilePath = Properties.backgroundClassifierLocation + 'background_' + dataset.name + '_' + 'DatasetBased_OperatorAssignmentBased' + '_classifier_obj'
        return backgroundFilePath

    def getClassificationModel(self, dataset: Dataset, backgroundFilePath: str):
        try:
            backgroundModel = Serializer.Deserialize(backgroundFilePath)
            return backgroundModel

        except Exception as ex:
            Logger.Error("getBackgroundClassificationModel -> Error reading classifier for dataset " + dataset.getName()
                         + "  from file: " + backgroundFilePath + "  :  " + ex)
            return None

    # return absolute path of the dataset
    def getOriginalBackgroundDatasets(self):
        datasetFolder = Properties.originalBackgroundDatasetsLocation
        datasetsFilesList = [os.path.join(datasetFolder, f) for f in os.listdir(datasetFolder) if os.path.isfile(os.path.join(datasetFolder, f))]
        if len(datasetsFilesList)==0:
            raise Exception('generateMetaFeaturesInstances -> no files in ' + datasetFolder)
        return datasetsFilesList


